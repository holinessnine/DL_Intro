{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab09b-transfer-CatsDogs.ipynb의 사본","provenance":[{"file_id":"1VtfL8IGxMOYrCg-HnLcWOdxbjrTRACzg","timestamp":1635608942396}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SeYUVY3LLGIh"},"source":["# Lab 9b – Custom Dataset & Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"y8P1_Yl_zQAI"},"source":["## Part A. MS Cats and Dogs\n","- Ensure that you **enable GPU** and work on Part A in one go to save time\n","- Download the zip file from this website to your local machine https://www.microsoft.com/en-us/download/details.aspx?id=54765\n","- Click on the red \"Download\" button to download it to your computer."]},{"cell_type":"markdown","metadata":{"id":"9B8oqmfl2U8l"},"source":["### Save Zip to Drive and Mount Drive\n","- Upload the zip file `kagglecatsanddogs_3367a.zip` onto a folder called \"DL\" in your Drive (might take a few minutes)\n","- You can name the folder anything you want but replace \"DL\" with that folder in the code cells below\n","- You will be asked for an authorisation code. Follow the link, sign in and copy the code\n","- You should see \"gdrive\" folder on the left pane\n","\n"]},{"cell_type":"code","metadata":{"id":"347JaRYM26cU"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xfSZto4f6PXT"},"source":["### Inspect the folder where the zip file is\n","- Replace `DL` with your own folder or remove it from that path if it is in your main Drive folder"]},{"cell_type":"code","metadata":{"id":"Cl93L1c64Ipu"},"source":["!ls '/content/gdrive/MyDrive/DL/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tFAn9k-L6ZIc"},"source":["### Copy it to your current directory\n","- When this code cell finishes, refresh the folder on the left pane and you should see the zip file in it"]},{"cell_type":"code","metadata":{"id":"Y_9oiQi54lOi"},"source":["!cp '/content/gdrive/MyDrive/DL/kagglecatsanddogs_3367a.zip' .\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xaxILizM6c0U"},"source":["### Unzip the zip file\n","- You should now see `PetImages` folder with `Cat` and `Dog` folders inside\n","- You may want to close the cell code output after it is done"]},{"cell_type":"code","metadata":{"id":"UZwJcxwn5VAX"},"source":["!unzip kagglecatsanddogs_3367a.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YWim5VPQrtmL"},"source":["import glob # Unix style pathname pattern matching\n","import cv2 # OpenCV\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn\n","import torchvision\n","from torchvision import models\n","\n","import torchvision.utils\n","from torch.utils.data import Dataset, DataLoader\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h69TsGycvJqy"},"source":["### Pre-processing: Organise Images Contained in Folders\n","- Normally training and test data are organised in SEPARATE directories so you can easily just call the constructor class on these separate folders\n","- However, we only have two folders containing **ALL images** of cats and dogs respectively and there are **NO labels** provided. The folder name is the indication of the class label.\n","- Often you might also work with **annotation files** that give textual references to which might be slightly easier to work with\n","- Here, we will go through all the image files contained in `PetImages` (and its subdirectories) and for each of them, store their path and class label in a list called **`init_data`**\n","- We will use *glob* and *OpenCV* to do these\n","- We will do the data splitting into train set and test set later"]},{"cell_type":"code","metadata":{"id":"Du2dc4xQq0Mn"},"source":["root_dir = \"PetImages/\"\n","\n","# Retrieve the list of ALL folders and files inside the base folder\n","file_list = glob.glob(root_dir + \"*\") # ['PetImages/Dog', 'PetImages/Cat']\n","\n","init_data = []\n","# Create a list that would contain the paths to all the images in the dataset\n","for class_path in file_list:\n","    class_name = class_path.split(\"/\")[-1] # extract the class name, e.g. 'PetImages/Dog' --> 'Dog'\n","    # Retrieve each image in their folders (i.e. 0.jpg, 1.jpg, etc.)\n","    for img_path in glob.glob(class_path + \"/*.jpg\"): # for each file with the extension \".jpg\"\n","        # Check for problematic images\n","        img = cv2.imread(img_path, cv2.IMREAD_COLOR)                 \n","        if type(img) is np.ndarray:\n","          if img.size == 0:\n","            continue\n","        if img is None:\n","            continue\n","        init_data.append([img_path, class_name]) # append image file path to init_data list, and its class name\n","\n","print(init_data) # [['PetImages/Dog/4.jpg', 'Dog'], ...]  \n","print(len(init_data))\n","\n","# Q1. Given that there are a total of 25,000 images in the folders, did all the images get saved into init_data? \n","# Explain why do you think not all images were loaded if that was the case.\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ESieonI6ap3"},"source":["### Display any one image"]},{"cell_type":"code","metadata":{"id":"HOb2SjmS6Zcr"},"source":["# Display any image with known name\n","from PIL import Image\n","\n","img = Image.open('PetImages/Cat/100.jpg')\n","plt.imshow(img)  \n","img.size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hPXB5hLOI_Al"},"source":["### Device Configuration and Hyper-parameters"]},{"cell_type":"code","metadata":{"id":"7HmdLeZgJFqf"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","# Hyper-parameters\n","SIZE = 224\n","MEAN = (0.485, 0.456, 0.406)\n","STD = (0.229, 0.224, 0.225)\n","BATCH = 4\n","EPOCHS = 1\n","LR = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lPC0ETrDwu1e"},"source":["### Create a Custom Dataset\n","- This class contains three main methods `__init__()`, `__len__()` and `__getitem__()`\n","- All three should be implemented although `__init__()` is not mandatory"]},{"cell_type":"code","metadata":{"id":"Q3WhwdUDrwmM"},"source":["class CustomCatDog(Dataset):\n","\n","    def __init__(self, data, transform=None):\n","        self.data = data\n","        # Dictionary containing mapping between string and number (class label)\n","        self.class_map = {\"Dog\" : 0, \"Cat\": 1} \n","        # Set image dimensions\n","        self.img_dim = (SIZE, SIZE) \n","        # Perform any transformations\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    # This function is called when an element of this class is accessed via dataset[index]\n","    def __getitem__(self, index):\n","        # Get image path and class name\n","        img_path, class_name = self.data[index] # tuple from __init__()\n","        # Obtain the image from its path and resize it to the required dimension\n","        img = cv2.imread(img_path, cv2.IMREAD_COLOR) \n","        img = cv2.resize(img, self.img_dim)\n","        class_id = self.class_map[class_name] # get the class label, e.g. 0 (for \"Dog\")\n","        pil_img=Image.fromarray(img) # convert ndarray to PIL\n","        # Perform transforms if there are any\n","        if self.transform:\n","            img_tensor = self.transform(pil_img)\n","        # Convert class label to a tensor\n","        class_id = torch.tensor(class_id) \n","\n","        return img_tensor, class_id\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X2IPHFYmqqUL"},"source":["### Transforms\n","- Training set and test set have slightly different transforms"]},{"cell_type":"code","metadata":{"id":"GtIx0kjEp2Vl"},"source":["import torchvision.transforms as transforms\n","\n","train_transforms = transforms.Compose([\n","        transforms.RandomResizedCrop(SIZE, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n","        transforms.RandomRotation(15),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(MEAN, STD)\n","    ])\n","\n","test_transforms = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(MEAN, STD)\n","    ])\n","\n","# Q2. Why is resizing not done in transforms?\n","\n","# Q3. Think of ONE more transform that could be applied on the train set. You DO NOT have to implement it.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RO5U3QXBw1Bd"},"source":["### Split Dataset into Training & Test Sets and Load"]},{"cell_type":"code","metadata":{"id":"5IpPPL1CuANm"},"source":["# Divide dataset into training and test sets using 80/20 ratio\n","train_size = int(0.8 * len(init_data))\n","test_size = len(init_data) - train_size\n","train_set, test_set = torch.utils.data.random_split(init_data, [train_size, test_size])\n","\n","trainset = CustomCatDog(train_set, transform=train_transforms)\n","\n","testset = CustomCatDog(test_set, transform=test_transforms)\n","\n","train_loader = DataLoader(trainset, batch_size=BATCH,\n","                          shuffle=True, num_workers=2, pin_memory=True)\n","\n","test_loader = DataLoader(testset, batch_size=BATCH,\n","                          shuffle=False, num_workers=2, pin_memory=True)\n","\n","\n","# Q4. What is the effect of splitting the data randomly and shuffling the data when loading?\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9LPRH_H5Fpa"},"source":["### Visualisation"]},{"cell_type":"code","metadata":{"id":"KpiZk2yhExPe"},"source":["# A function to denormalise an image for better visualisation\n","def inverse_normalize(tensor, mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)):\n","    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device)\n","    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device)\n","    if mean.ndim == 1:\n","        mean = mean.view(-1, 1, 1)\n","    if std.ndim == 1:\n","        std = std.view(-1, 1, 1)\n","    tensor.mul_(std).add_(mean)\n","    return tensor\n","\n","# Get a batch of images from the train loader\n","batch = next(iter(train_loader))\n","\n","images, labels = batch\n","\n","print(images.shape)\n","\n","images = inverse_normalize(tensor=images, mean=MEAN, std=STD)  \n","\n","# Create a grid \n","plt.figure(figsize=(12,12))\n","grid = torchvision.utils.make_grid(tensor=images, nrow=4) # nrow = number of images displayed in each row\n","\n","print(f\"class labels: {labels}\")\n","\n","# Use grid.permute() to transpose the grid so that the axes meet the specifications required by \n","# plt.imshow(), which are [height, width, channels]. PyTorch dimensions are [channels, height, width].\n","plt.imshow(grid.permute(1,2,0))\n","\n","# Q5. What are the values passed into inverse_normalize() for mean and std in this code cell?\n","\n","# Q6. Where are these displayed images stored?\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"27IvhTawGqK6"},"source":["### Load a Pre-trained Model"]},{"cell_type":"code","metadata":{"id":"U_AceMgbHs7p"},"source":["from torchvision import models\n","\n","# VGG16 Model Loading\n","use_pretrained = True\n","model = models.vgg16(pretrained=use_pretrained)\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tkae3_ofmfHs"},"source":["# Q7. Why does the final output layer of VGG16 contain 1000 units?\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmEYW3fCR7SF"},"source":["### Get final layer name"]},{"cell_type":"code","metadata":{"id":"v3Y9WhpJR-NF"},"source":["# Run this code cell to see all the names of the layers\n","for n, m in model.named_modules():\n","     m.auto_name = n\n","     print(n)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KnR7s9BbIDzP"},"source":["### Make modifications to the Final Output Layer Only"]},{"cell_type":"code","metadata":{"id":"n_0pSvbcIGlk"},"source":["# Change the Last Layer, classifier[6]\n","# Output Features 1000 -> 2\n","model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=2)\n","print('Done')\n","\n","# Q8. Why are there 2 units in the final output layer now? What type of task is the network trying to learn?\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UnTB-OE4Ia3K"},"source":["### Specify Layers for Parameter Updates"]},{"cell_type":"code","metadata":{"id":"VdCw78YBIbVR"},"source":["# Create list\n","params_to_update = []\n","\n","update_params_name = ['classifier.6.weight', 'classifier.6.bias']\n","\n","for name, param in model.named_parameters():\n","    if name in update_params_name:\n","        param.requires_grad = True\n","        params_to_update.append(param)\n","        print(name)\n","    else:\n","        param.requires_grad = False\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1682z9lQ3L0W"},"source":["# Q9. How many layers will be retrained according to the code?\n","\n","# Q10. Which layers will be retrained?\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N3O889CaIs8d"},"source":["### Loss and Optimiser"]},{"cell_type":"code","metadata":{"id":"B3tPA6fsIvCI"},"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","# Important! Place model on GPU BEFORE optimiser is initialised\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(params=params_to_update, lr=0.0005)\n","\n","# Q11. Looking at the loss function, what output activation function will this neural network have?\n","\n","# Q12. Why is the first argument to the optimiser not model.parameters()?"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sCwDu413J34T"},"source":["### Training Loop\n","- Function `train()` takes in the NN model, `net`, the training data loader, `dataloader`, the test_loader, `val_dataloader`, optimiser and loss\n"]},{"cell_type":"code","metadata":{"id":"rXg-3CtyRSzJ"},"source":["import time\n","\n","n_total_steps = len(train_set)\n","n_iterations = -(-n_total_steps // BATCH) # ceiling division\n","print(f'Total steps: {n_total_steps}')\n","print(f'Iterations per epoch: {n_iterations}')\n","\n","def test(net, dataloader):\n","    correct = 0\n","    total = 0\n","    net.eval()\n","    with torch.no_grad():\n","        for data in dataloader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            acc = 100 * correct / total\n","    print(f'Val Accuracy: {acc:.2f}')\n","    net.train()\n","    return round(acc, 2)\n","\n","def train(net, dataloader, test_dataloader, optimizer, criterion):\n","    since = time.time() # start time\n","\n","    num_epochs = 1\n","    net.train()\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        for i, data in enumerate(dataloader, 0):\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            freq = 50\n","            if ((i+1) % freq) == 0:\n","                print(f'Epoch {epoch + 1} Iteration {i + 1}/{n_iterations} Train Loss: {(running_loss / freq):.4f}')\n","                running_acc = test(net, test_dataloader)\n","                if (running_acc >= 97.75): # stop when 97.75% accuracy is reached\n","                  break\n","                running_loss = 0.0\n","\n","    time_elapsed = time.time() - since\n","    print(f'Finished Training {time_elapsed}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m3FwEzLMk-qY"},"source":["### Call the Train Function"]},{"cell_type":"code","metadata":{"id":"7n0BiRkjRZSu"},"source":["train(model, train_loader, test_loader, optimizer, criterion)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k98Bll2HRFqX"},"source":["# Q13. What was the accuracy on the test set after the first 50 iterations?\n","\n","# Q14. What explains the high accuracy after just the first 50 iterations?\n","\n","# Q15. Why is training still relatively slow using a small batch size of 4?\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gq5wPB454sH3"},"source":["### Exercise: Use Any Other Pre-trained Model\n","- Try to load a different pre-trained model and modify just the final output layer to be retrained with the cats and dogs custom dataset\n","- A list of all available models is here https://pytorch.org/vision/stable/models.html\n","- Try one of these: `resnet18, inception, squeezenet, googlenet` or any other model of your choice\n"]},{"cell_type":"markdown","metadata":{"id":"wmfy7jeBiPpz"},"source":["### Part B. Bees and Ants (ImageFolder)\n","- Simple illustration of using ImageFolder for loading images"]},{"cell_type":"code","metadata":{"id":"ftKsOsYRgube"},"source":["import torch\n","import torchvision.transforms as transforms\n","\n","import os\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XF4aXFisifht"},"source":["# Q16. Download the zip file from here https://download.pytorch.org/tutorial/hymenoptera_data.zip\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OYYHcXZbkbMj"},"source":["## Unzip the file"]},{"cell_type":"code","metadata":{"id":"X3CSQZuhkc9_"},"source":["# Q17. Unzip the file\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2KV_MeOlKJ6"},"source":["# Inspect the folder where the files are unzipped to and answer the following questions\n","\n","# Q18. What is the name of the main (root) directory of this dataset?\n","\n","# Q19. Is this data already organised into training and validation sets?\n","\n","# Q20. What are the number of classes in this dataset and the class names?\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gt0w-nvMl6Cq"},"source":["# Q21. Set the data directory to the root directory\n","# data_dir =\n","\n","# Q22. Set the number of classes\n","# num_classes = "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vjd1lPsyjqBc"},"source":["### Use ImageFolder and DataLoader\n","- When the data is already organised into train and test sets, `torchvision.datasets.ImageFolder` class can be used to directly store the data and also perform transforms."]},{"cell_type":"code","metadata":{"id":"lgiCcXrNw1Yp"},"source":["from torchvision.datasets import DataLoader, ImageFolder\n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","\n","image_datasets = {x: ImageFolder(os.path.join(data_dir, x),\n","                                 data_transforms[x])\n","                  for x in ['train', 'val']}\n","\n","dataloaders = {x: DataLoader(image_datasets[x], \n","                             batch_size=4,\n","                             shuffle=True)\n","              for x in ['train', 'val']}\n","\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","\n","class_names = image_datasets['train'].classes\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kCvF9OLdhCx0"},"source":["### Visualise the Images"]},{"cell_type":"code","metadata":{"id":"1NTS-F8n9xHK"},"source":["inputs, labels = next(iter(dataloaders['train']))\n","grid_images = torchvision.utils.make_grid(inputs)\n","\n","def no_normalize(im):\n","    im = im.permute(1, 2, 0)\n","    im = im*torch.Tensor([0.229, 0.224, 0.225])+torch.Tensor([0.485, 0.456, 0.406])\n","    return im\n","\n","\n","grid_images = no_normalize(grid_images)\n","plt.figure(figsize = (10,10))\n","plt.title([class_names[x] for x in labels])\n","plt.imshow(grid_images)\n","\n","plt.show()"],"execution_count":null,"outputs":[]}]}